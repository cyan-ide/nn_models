# nn_models
Neural network / AI models

My experiments with various neural network models via coding their implementations from scratch.

- transformer - everything from zero, following excelent tutorial by Peter Bloem
- GPT-2 - adapted the earlier done transformer code for Masked Attention and later followed Karpathy awesome tutorial on writing GPT-2 from scratch
- BERT - took what I learned from earlier GPT-2 tutorial and replicated the entire process to write BERT from scratch. Quite a bit harder, as there aren't any tutorials available that would go all the way with pre-training. Most publications/videos/code just play with tiny data or dont really follow through til the end to show the results.
